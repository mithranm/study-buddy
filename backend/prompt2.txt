├── .pytest_cache
│   ├── v
│   │   └── cache
│   │       ├── lastfailed
│   │       ├── nodeids
│   │       └── stepwise
│   ├── CACHEDIR.TAG
│   └── README.md
├── chroma_data
│   └── chroma.sqlite3
├── chroma_db
├── extracted_images
│   ├── c33595b6-75ea-4239-97c6-95f09e44306f.jpeg
│   └── e420292d-62dc-4e71-93f8-c3d6b9f41c85.jpeg
├── logs
│   ├── document_content.log
│   ├── integration_test_essay_content.log
│   ├── test_content.log
│   └── vision-llm_content.log
├── secrets
│   ├── secrets-go-here.json
│   └── selfcareassistant-99a02733eae9.json
├── src
│   ├── .DS_Store
│   ├── __init__.py
│   ├── document_chunker.py
│   ├── document_textractor.py
│   ├── google_calls.py
│   ├── main.py
│   ├── make_celery.py
│   ├── ollama_calls.py
│   ├── pdf_extractor.py
│   ├── tasks.py
│   ├── vector_db.py
│   └── wsgi.py
├── test_images
│   └── mystery.jpeg
├── test_upload
│   ├── .DS_Store
│   ├── document.pdf
│   ├── test.pdf
│   └── vision-llm.pdf
├── tests
│   ├── __init__.py
│   ├── test_integration.py
│   └── test_main.py
├── textracted
├── uploads
│   └── vision-llm.pdf
├── .DS_Store
├── .dockerignore
├── .env
├── .python-version
├── Dockerfile
├── example.env
├── permissions.txt
├── poetry.lock
├── prompt.txt
├── prompt2.txt
├── pydoc-markdown.yml
├── pyproject.toml
├── run-dev-server.sh
├── run_task_queue.sh
├── test_integration.log
└── test_main.log


main.py
```
import os
import nltk
import logging
import traceback
import sys
import time
from flask import Flask, request, jsonify, Blueprint, current_app
from flask_cors import CORS
from celery import Celery, Task
from celery.concurrency import asynpool
import redis

asynpool.PROC_ALIVE_TIMEOUT = 60.0  # Prevents deadlock issues

# Project python files.
from . import document_chunker as chunker
from . import vector_db
from . import ollama_calls as ollama
from .tasks import process_file

# BLUEPRINT OF API
bp = Blueprint('study-buddy', __name__)

logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

# Set urllib3 logger to WARNING level
urllib3_logger = logging.getLogger('urllib3')
urllib3_logger.setLevel(logging.WARNING)

# Set Gunicorn logger to WARNING level (if you're using Gunicorn)
gunicorn_logger = logging.getLogger('gunicorn.error')
gunicorn_logger.setLevel(logging.WARNING)

# API ENDPOINT METHODS HERE
@bp.route('/status', methods=['GET'])
def get_status():
    """
    Getter method for status of backend to let any application using this interface know when its ready.

    Args:
        None

    Returns:
        json - containing nltk and chroma status variables.
    """
    return jsonify({
        'nltk_ready': current_app.nltk_ready,
        'chroma_ready': current_app.chroma_ready,
        'error': current_app.initialization_error
    })

@bp.route('/upload', methods=['POST'])
def upload_file():
    """
    Uploads a file that is chosen by the user.

    This function handles all POST request to the '/upload' endpoint.

    Args:
        None
    Returns:
        tuple: a json of the message and the http code
        if successful: ({'message': 'File uploaded and embedded sucessfully'}, 200)
        if backend not ready: ({'error': 'Backend is not fully initialized yet'}, 503)
        if theres no file to upload: ({'error': 'No file part'}, 400)
        if filename is empty: ({'error': 'No selected file'}, 400)
    """
    if 'file' not in request.files:
        return jsonify({'error': 'No file part'}), 400
    file = request.files['file']
    if file.filename == '':
        return jsonify({'error': 'No selected file'}), 400
    if file:
        file_path = os.path.join(current_app.config['UPLOAD_FOLDER'], file.filename)

        collection = vector_db.get_collection()
        # Checks to see if the file already exists in the upload directory to prevent from the file being chunked again in chroma db.

        if(os.path.exists(file_path) and len(collection.get(where={"source": file_path})['ids'])): # Also checks the data base just to make sure no funny business is going on
            return jsonify({"error": "Tried uploading a file that already exists."}), 400
        
        file.save(file_path)

        # Sending a task to complete adding to db in the background.
        process_file.delay(file.filename, file_path, current_app.config["TEXTRACTED_PATH"])

        return jsonify({'message': 'File recieved and is being processed', 'filename': file.filename}), 202

@bp.route('/search', methods=['POST'])
def search_wrapper():
    """
    Search through submitted files to find the best matches for the given query. This is a wrapper to vector_db.search_documents()

    This function handles all POST requests to the '/search' endpoint.

    Args:
        None
    Returns:
        tuple - a json of the data and the http code
        if backend not ready: returns ({'error': 'Backend is fully initialized yet'}), 503)
    """
    query = request.json.get('query')
    if not query:
        return jsonify({'error': 'No query provided'}), 400
    return vector_db.search_documents(query)

@bp.route('/documents', methods=['GET'])
def list_documents():
    """
    Retrieve all documents and return it back to the frontend in json format to be displayed on the screen.

    This function handles all GET requests to '/documents' endpoint.

    Args:
        None
    Returns:
        tuple: a json file that contains data and http code
        if successful: sends a json of the files submitted with the http code 200.
        if backend not ready: ({error: Backend is fully initialized yet}, 503).

    Raises:
        None
    """
    files = os.listdir(current_app.config['UPLOAD_FOLDER'])
    return jsonify(files)

@bp.route('/documents/<filename>', methods=['DELETE'])
def delete_document(filename):
    """
    Delete a document and its associated chunks from the system.

    This function handles HTTP DELETE requests to remove a specific document
    identified by its filename. It also removes the corresponding document
    chunks from the Chroma vector store.

    Args:
        filename (str): The name of the file to be deleted.

    Returns:
        tuple: A tuple containing a JSON response and an HTTP status code.
            - If successful: ({'message': 'Document deleted successfully'}, 200)
            - If backend not ready: ({'error': 'Backend is not fully initialized yet'}, 503)
            - If file not found: ({'error': 'Document not found'}, 404)

    Raises:
        None
    """
    file_path = os.path.join(current_app.config['UPLOAD_FOLDER'], filename) 
    if os.path.exists(file_path):
        # Remove document chunks from Chroma
        collection = vector_db.get_collection()
        collection.delete(where={"source": file_path})

        # Remove from backend/upload/
        os.remove(file_path)
        
        split_filename = os.path.splitext(filename)
        if (split_filename[1] == ".pdf"):
            textracted_path = os.path.join(current_app.config['TEXTRACTED_PATH'], f"{split_filename[0]}.md")

            os.remove(textracted_path)

        logger.info(f"New collection file should not be present: {collection.peek(100)['ids']}") # after deletion you should not see the file in the database you just deleted.
        return jsonify({'message': 'Document deleted successfully'}), 200
    else:
        return jsonify({'error': 'Document not found'}), 404
    
@bp.route('/get_models', methods=['GET'])
def get_models_wrapper():
    """
    Add pydocs here :p
    """
    if (not ollama.ollama_health_check()):
        logger.info("Ollama is not running")
        return jsonify({'error': 'Ollama is not running, please make sure ollama is running on your local machine'}, 503)
    
    return ollama.get_models()

@bp.route('/chat', methods=['POST'])
def chat_wrapper():
    """
    chat an LLM response from the prompt in the request body and chunks from documents already uploaded.

    Args:
        None

    Returns:
        tuple: A tuple containing a JSON response and an HTTP status code.
    Raises:
        None
    """

    if (not ollama.ollama_health_check()):
        logger.info("Ollama is not running")
        return jsonify({'error': 'Ollama is not running, please make sure ollama is running on your local machine'}, 503)
    
    prompt = request.json.get('prompt')

    model = request.json.get('model')

    if not prompt:
        return jsonify({'error': 'No prompt given'}), 400

    try:
        print("running search documents")
        search_results, http_code = vector_db.search_documents(prompt)

        if http_code != 200:
            return jsonify({'error': 'Search failed'}), http_code
        
        # Call chat with the search results and prompt
        return ollama.chat(search_results, prompt, model)
    except Exception as e:
        print("Exception chat")
        traceback.print_exc(file=sys.stderr)
        return jsonify({'error': f'Exception in chat process: {str(e)}'}), 500
    
def celery_init_app(app: Flask) -> Celery:
    """
    Initialize celery app with threading configuration.
    """
    class FlaskTask(Task):
        def __call__(self, *args: object, **kwargs: object) -> object:
            with app.app_context():
                return self.run(*args, **kwargs)

    celery_app = Celery(app.name, task_cls=FlaskTask)
    celery_app.conf.update(
        broker_url="redis://localhost:6379",
        result_backend="redis://localhost:6379",
        task_ignore_result=True,
        worker_prefetch_multiplier=1,
        worker_concurrency=1,
        worker_pool='threads',
        task_acks_late=True,
        worker_max_tasks_per_child=1000,  # Restart worker after 1000 tasks
    )
    celery_app.set_default()
    app.extensions["celery"] = celery_app
    return celery_app

def create_app(test_config=None):
    """
    Creates the app.
    """
    app = Flask(__name__)
    CORS(app)

    # Initialize Redis client
    app.redis_client = redis.StrictRedis(
        host='localhost',   # Change as necessary
        port=6379,          # Default Redis port
        db=0,               # Database number
        decode_responses=True  # Optional: decode responses to strings
    )

    app.config.from_mapping(
        CHROMA_SERVER_HOST="chromadb",  # Service name if using Docker Compose
        CHROMA_SERVER_PORT=9092,        # Port exposed by ChromaDB server
    )

    app.config.from_prefixed_env()
    celery_init_app(app)

    if test_config is None:
        # Load the instance config, if it exists, when not testing
        app.config.from_pyfile('config.py', silent=True)
    else:
        # Load the test config if passed in
        app.config.from_mapping(test_config)

    # Set base_path to the project root directory
    base_path = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))

    # Set default configurations
    app.config.setdefault('CHROMA_DB_PATH', os.path.join(base_path, 'chroma_db'))
    app.config.setdefault('UPLOAD_FOLDER', os.path.join(base_path, 'uploads'))
    app.config.setdefault('TEXTRACTED_PATH', os.path.join(base_path, 'textracted'))

    # Log all app configurations
    for key, value in app.config.items():
        logger.debug(f"App config: {key} = {value}")

    # Ensure the required directories exist
    try:
        os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)
        os.makedirs(app.config['CHROMA_DB_PATH'], exist_ok=True)
        os.makedirs(app.config['TEXTRACTED_PATH'], exist_ok=True)
        logger.info("Created necessary directories")
    except Exception as e:
        logger.error(f"Error creating directories: {str(e)}")

    # Global variables to track initialization status
    app.nltk_ready = False
    app.chroma_ready = False
    app.initialization_error = None

    with app.app_context():
        try:
            # Initialize NLTK
            nltk.download('punkt_tab')
            app.nltk_ready = True

            # Initialize Chroma collection
            vector_db.get_collection()
            app.chroma_ready = True
        except LookupError as le:
            app.initialization_error = f"NLTK LookupError: {str(le)}"
            logger.error(f"Initialization error: {str(le)}")
        except Exception as e:
            app.initialization_error = f"General Initialization Error: {str(e)}"
            logger.error(f"Initialization error: {str(e)}")

    app.register_blueprint(bp, url_prefix="/api/")

    return app

if __name__ == '__main__':
    app = create_app()
    port = int(os.environ.get('FLASK_RUN_PORT', 9090))
    app.run(host='0.0.0.0', port=port, debug=False)
```

tasks.py



Error:

DEBUG:src.vector_db:Searching documents with query: hello
INFO:src.vector_db:CALLED GET_COLLECTION
INFO:src.vector_db:GET_CHROMA_CLIENT PASSED
DEBUG:chromadb.api.segment:Collection documents already exists, returning existing collection.
INFO:src.vector_db:GET_COLLECTION PASSED
DEBUG:chromadb.utils.embedding_functions.onnx_mini_lm_l6_v2:WARNING: No ONNX providers provided, defaulting to available providers: ['CoreMLExecutionProvider', 'AzureExecutionProvider', 'CPUExecutionProvider']
2024-10-05 20:24:51.665814 [E:onnxruntime:, sequential_executor.cc:516 ExecuteKernel] Non-zero status code returned while running CoreML_16544053425081978351_4 node. Name:'CoreMLExecutionProvider_CoreML_16544053425081978351_4_4' Status Message: Error executing model: Error in dynamically resizing for sequence length (error: -6).
ERROR:src.vector_db:Error during search: [ONNXRuntimeError] : 1 : FAIL : Non-zero status code returned while running CoreML_16544053425081978351_4 node. Name:'CoreMLExecutionProvider_CoreML_16544053425081978351_4_4' Status Message: Error executing model: Error in dynamically resizing for sequence length (error: -6).
